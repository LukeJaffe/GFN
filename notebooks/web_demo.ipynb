{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "06dc0801",
   "metadata": {},
   "source": [
    "# Gallery Filter Network Demo\n",
    "This notebook implements a demo showcasing the SeqNeXt person search model and Gallery Filter Network (GFN) scoring process.\n",
    "\n",
    "The notebook loads images from the web, and you can easily try it out on other image URLs.\n",
    "\n",
    "All dependencies are imported below, and the model is loaded via torchscript: our package is not used to make the demo (somewhat) self-contained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2ddab83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Torch libs\n",
    "import torch\n",
    "## Disable nvfuser for now\n",
    "torch._C._jit_override_can_fuse_on_cpu(False)\n",
    "torch._C._jit_override_can_fuse_on_gpu(False)\n",
    "torch._C._jit_set_texpr_fuser_enabled(False)\n",
    "torch._C._jit_set_nvfuser_enabled(False)\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Libs for data pre-processing\n",
    "import cv2\n",
    "import numpy as np\n",
    "from albumentations.augmentations.geometric import functional as FGeometric\n",
    "import torchvision.transforms.functional as TF\n",
    "\n",
    "# Libs for loading images\n",
    "import os\n",
    "import urllib.request\n",
    "from PIL import Image\n",
    "import ssl\n",
    "## Avoid SSL error\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "# Libs for visualization\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Rectangle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd4ea123",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f418e30a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download images from the web\n",
    "def load_web_image(\n",
    "        demo_image_dir='./demo_images',\n",
    "        image_url='https://pics.filmaffinity.com/Friends_TV_Series-783758929-large.jpg',\n",
    "        file_name='demo.png',\n",
    "        display=True,\n",
    "    ):\n",
    "    # Make dir to store the images\n",
    "    if not os.path.exists(demo_image_dir):\n",
    "        os.makedirs(demo_image_dir)\n",
    "\n",
    "    # Download the image URL from the web\n",
    "    urllib.request.urlretrieve(\n",
    "        image_url,\n",
    "        f'{demo_image_dir}/{file_name}')\n",
    "\n",
    "    # Load the image from disk\n",
    "    img = Image.open(f'{demo_image_dir}/{file_name}').convert('RGB')\n",
    "    \n",
    "    # Plot the image and show dimensions\n",
    "    if display:\n",
    "        fix, ax = plt.subplots(figsize=(6, 6))\n",
    "        ax.imshow(img)\n",
    "        plt.show()\n",
    "    \n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5b6a64d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert PIL image to torch tensor\n",
    "def to_tensor(image):\n",
    "    arr = np.array(image)\n",
    "    arr_wrs = window_resize(arr)\n",
    "    tsr = torch.FloatTensor(arr_wrs)\n",
    "    tsr_norm = normalize(tsr)\n",
    "    tsr_input = tsr_norm.permute(2, 0, 1).to(device)\n",
    "    return tsr_input\n",
    "\n",
    "# Convert torch tensor to PIL image\n",
    "def to_image(tensor):\n",
    "    tsr_denorm = denormalize(tensor.permute(1, 2, 0).cpu()).clip(min=0, max=1)\n",
    "    arr = tsr_denorm.numpy()\n",
    "    arr_uint8 = (arr * 255.0).astype(np.uint8)\n",
    "    image = Image.fromarray(arr_uint8)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f90c6569",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize image tensor using ImageNet stats\n",
    "def normalize(tensor, mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)):\n",
    "    mean = torch.FloatTensor(mean).view(1, 1, 3)\n",
    "    std = torch.FloatTensor(std).view(1, 1, 3)\n",
    "    return tensor.div(255.0).sub(mean).div(std)\n",
    "\n",
    "# Denormalize image tensor using ImageNet stats\n",
    "def denormalize(tensor, mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)):\n",
    "    mean = torch.FloatTensor(mean).view(1, 1, 3)\n",
    "    std = torch.FloatTensor(std).view(1, 1, 3)\n",
    "    return tensor.mul(std).add(mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34671fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resize image (numpy array) to fit in fixed size window\n",
    "def window_resize(img, min_size=900, max_size=1500, interpolation=cv2.INTER_LINEAR):\n",
    "    height, width = img.shape[:2]\n",
    "    image_min_size = min(width, height)\n",
    "    image_max_size = max(width, height)\n",
    "    scale_factor = min_size / image_min_size\n",
    "    if image_max_size * scale_factor > max_size:\n",
    "        return FGeometric.longest_max_size(img, max_size=max_size, interpolation=interpolation)\n",
    "    else:\n",
    "        return FGeometric.smallest_max_size(img, max_size=min_size, interpolation=interpolation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82a8b12a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot detected boxes on image with matplotlib\n",
    "def show_detects(image, detect, person_sim=None, show_detect_score=False, ax=None, title=None, xlabel=None):\n",
    "    # Setup subplot\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots(figsize=(6, 6))\n",
    "    # Setup labels\n",
    "    if title is not None:\n",
    "        ax.set_title(title, fontsize=20, fontweight='bold')\n",
    "    if xlabel is not None:\n",
    "        ax.set_xlabel(xlabel, fontsize=20)\n",
    "    # Show the image\n",
    "    ax.imshow(denormalize(image.permute(1, 2, 0).cpu()))\n",
    "    # Plot boxes (and optionally similarity scores)\n",
    "    for i, (box, score) in enumerate(zip(detect['det_boxes'].cpu().tolist(), detect['det_scores'].cpu().tolist())):\n",
    "        x, y, x2, y2 = box\n",
    "        w, h = x2 - x, y2 - y\n",
    "        ax.add_patch(Rectangle((x, y), w, h, edgecolor='green', lw=4, fill=False, alpha=0.8))\n",
    "        ax.add_patch(Rectangle((x+2, y+2), w-4, h-4, edgecolor='whitesmoke', lw=1, fill=False, alpha=0.8))\n",
    "        ## Display person similarity if this is supplied\n",
    "        if person_sim is not None:\n",
    "            ax.text(x, y, '{:.2f}'.format(person_sim[i].item()), ha=\"left\", va=\"bottom\", size=14,\n",
    "                bbox=dict(boxstyle=\"square,pad=0.2\", fc=\"whitesmoke\", alpha=0.8, ec='black', lw=2.0)\n",
    "            )\n",
    "        ## Otherwise, display detected box scores\n",
    "        elif show_detect_score:\n",
    "            ax.text(x, y, '{:.2f}'.format(detect['det_scores'][i].item()), ha=\"left\", va=\"bottom\", size=14,\n",
    "                bbox=dict(boxstyle=\"square,pad=0.2\", fc=\"lightblue\", alpha=0.8, ec='black', lw=2.0)\n",
    "            ) \n",
    "    # Remove ticks and expand borders\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    [x.set_linewidth(3) for x in ax.spines.values()]\n",
    "\n",
    "# Return list of detected (PIL) image crops\n",
    "def get_crops(tensor, detect, ax=None):\n",
    "    # Convert tensor back to image\n",
    "    image = to_image(tensor)\n",
    "    # Extract crops using detected boxes\n",
    "    crop_list = []\n",
    "    for i, box in enumerate(detect['det_boxes'].cpu().tolist()):\n",
    "        x1, y1, x2, y2 = box\n",
    "        crop = image.crop((x1, y1, x2, y2))\n",
    "        crop_list.append(crop)\n",
    "    return crop_list\n",
    "\n",
    "# Show ranked re-id crops\n",
    "def show_reid(crop_list, score_list, plot_width=0.75):\n",
    "    # Sort crops by decreasing score\n",
    "    sorted_score_idx = np.argsort(score_list)[::-1]\n",
    "    sorted_score_list = [score_list[i] for i in sorted_score_idx]\n",
    "    sorted_crop_list = [crop_list[1:][i] for i in sorted_score_idx]\n",
    "    # Plotting helper function\n",
    "    def _plot_subplot(_ax, title='', fw=None):\n",
    "        _ax.set_title(title, fontweight=fw, fontsize=11)\n",
    "        _ax.set_xticks([])\n",
    "        _ax.set_yticks([])\n",
    "        [x.set_linewidth(2) for x in _ax.spines.values()]\n",
    "     # Plot query crop\n",
    "    fig, ax = plt.subplots(nrows=1, ncols=len(crop_list), figsize=(plot_width*len(crop_list), plot_width*3))\n",
    "    ax[0].imshow(crop_list[0].resize((100, 300)))\n",
    "    _plot_subplot(ax[0], title='Query', fw='bold')\n",
    "    # Plot gallery crops\n",
    "    for i, (crop, score) in enumerate(zip(sorted_crop_list, sorted_score_list), 1):\n",
    "        ax[i].imshow(crop.resize((100, 300)))\n",
    "        _plot_subplot(ax[i], title='s={:.2f}'.format(score))\n",
    "        ax[i].set_xticks([])\n",
    "        ax[i].set_yticks([])\n",
    "        [x.set_linewidth(2) for x in ax[i].spines.values()]\n",
    "    # Return fig\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d5fb887",
   "metadata": {},
   "source": [
    "## User Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc7b6bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "torchscript_path = '../torchscript/cuhk_final_convnext-base_e30.torchscript.pt'\n",
    "\n",
    "# Device\n",
    "device = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e61fe4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Query data\n",
    "query_list = [\n",
    "    {\n",
    "        # Chandler \n",
    "        'url': 'https://i.redd.it/uqrvlpf667j51.jpg',\n",
    "        'file': 'chandler_query.jpg',\n",
    "    },\n",
    "    {\n",
    "        # Ross \n",
    "        'url': 'https://metro.co.uk/wp-content/uploads/2019/04/dr3-f247.jpg?quality=90&strip=all&zoom=1&resize=644%2C367',\n",
    "        'file': 'ross_query.jpg',\n",
    "    }\n",
    "]\n",
    "\n",
    "# Gallery data\n",
    "gallery_list = [\n",
    "    {\n",
    "        'url': 'https://deadline.com/wp-content/uploads/2022/10/Screenshot-2022-10-22-at-09.45.12.png?w=681&h=383&crop=1',\n",
    "        'file': 'friends_gallery3.jpg',\n",
    "    },\n",
    "    {\n",
    "        'url': 'https://static.wikia.nocookie.net/friends/images/0/03/TOWChandler%27sWorkLaugh.png/revision/latest/scale-to-width-down/1000?cb=20180307145208',\n",
    "        'file': 'friends_gallery4.jpg',\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9fb9cab",
   "metadata": {},
   "source": [
    "## Image Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6d89b2e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Load queries\n",
    "print('Loading query images:')\n",
    "query_image_list = []\n",
    "for query_dict in query_list:\n",
    "    query_image = load_web_image(image_url=query_dict['url'], file_name=query_dict['file'])\n",
    "    ## Take center crop of query image to make display look better\n",
    "    query_image = TF.center_crop(query_image, min(query_image.size))\n",
    "    query_image_list.append(query_image)\n",
    "\n",
    "# Load gallery\n",
    "print('Loading gallery images:')\n",
    "gallery_image_list = []\n",
    "for gallery_dict in gallery_list:\n",
    "    gallery_image = load_web_image(image_url=gallery_dict['url'], file_name=gallery_dict['file'])\n",
    "    gallery_image_list.append(gallery_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86104901",
   "metadata": {},
   "source": [
    "## Model Loading\n",
    "\n",
    "Loading the torchscript model is just a single function call: no libs or configs needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce914d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load torchscript version of model\n",
    "model = torch.jit.load(torchscript_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6647cbf",
   "metadata": {},
   "source": [
    "## Run Model on Query Scenes: Get Query Person and Scene Embeddings\n",
    "\n",
    "There are two ways get query embeddings:\n",
    "1. Let the model try to detect the query bounding box, and use the detected embedding.\n",
    "2. Input the query bounding box explicitly. If the query scene just has one person, you can also supply the full scene extent as the bounding box.\n",
    "\n",
    "Both methods 1. and 2. are done below as an example, but only the detected embedding is used later in the demo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d147949",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put query sample through model\n",
    "query_output_list = []\n",
    "with torch.no_grad():\n",
    "    query_tensor_list = [to_tensor(query_image) for query_image in query_image_list]\n",
    "    for query_tensor in query_tensor_list:\n",
    "        ## Use the full scene extent as a bounding box\n",
    "        query_box = torch.FloatTensor([0, 0, *query_tensor.shape[1:]]).unsqueeze(0).to(device)\n",
    "        query_targets = [{'boxes': query_box}]\n",
    "        ## Run query scene through the model\n",
    "        detections = model([query_tensor], query_targets, inference_mode='both')\n",
    "        ## Reorganize results\n",
    "        for query_image, query_detect in zip([query_tensor], detections):\n",
    "            ## Show detected boxes with their scores\n",
    "            show_detects(query_tensor, query_detect, show_detect_score=True)        \n",
    "            query_output_list.append(query_detect)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f7988ac",
   "metadata": {},
   "source": [
    "## Run Model on Gallery Scenes: Get Gallery Person and Scene Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95c7c274",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Put gallery samples through model\n",
    "gallery_output_list = []\n",
    "with torch.no_grad():\n",
    "    gallery_tensor_list = [to_tensor(image) for image in gallery_image_list]\n",
    "    ## Run query scene through the model\n",
    "    detections = model(gallery_tensor_list, inference_mode='det')\n",
    "    ## Reorganize results\n",
    "    for tensor, detect in zip(gallery_tensor_list, detections):\n",
    "        ## Show detected boxes with their scores\n",
    "        show_detects(tensor, detect, show_detect_score=True)\n",
    "        gallery_output_list.append(detect)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c540ec23",
   "metadata": {},
   "source": [
    "## Get Re-ID Scores: Compare Person Embeddings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec2c6213",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each query image\n",
    "for query_detect in query_output_list:\n",
    "    # Get query person embeddings\n",
    "    query_person_emb = query_detect['det_emb']\n",
    "\n",
    "    # For each gallery image\n",
    "    for gallery_output_dict in gallery_output_list:\n",
    "        ## Get gallery person embeddings\n",
    "        gallery_person_emb = gallery_output_dict['det_emb']\n",
    "\n",
    "        ## Compute person similarity: cosine similarity of person embeddings\n",
    "        person_sim = torch.mm(\n",
    "            F.normalize(query_person_emb, dim=1),\n",
    "            F.normalize(gallery_person_emb, dim=1).T\n",
    "        ).flatten()\n",
    "        \n",
    "        ## Store person similarity\n",
    "        if 'person_sim' not in gallery_output_dict:\n",
    "            gallery_output_dict['person_sim'] = []\n",
    "        gallery_output_dict['person_sim'].append(person_sim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9759f678",
   "metadata": {},
   "source": [
    "## Get GFN Scores: Compare Query-Gated Scene Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4611554c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each query image\n",
    "for query_detect in query_output_list:\n",
    "    # Get query person and scene embeddings\n",
    "    query_person_emb = query_detect['det_emb']\n",
    "    query_scene_emb = query_detect['scene_emb']\n",
    "\n",
    "    # For each gallery image\n",
    "    for gallery_output_dict in gallery_output_list:\n",
    "        ## Get gallery scene embeddings\n",
    "        gallery_scene_emb = gallery_output_dict['scene_emb']\n",
    "\n",
    "        ## Compute query-scene similarity: cosine similarity of query-gated scene embeddings\n",
    "        with torch.no_grad():\n",
    "            qg_scene_sim = model.gfn.get_scores(query_person_emb, query_scene_emb, gallery_scene_emb).flatten().item()\n",
    "        \n",
    "        ## Store query-scene similarity\n",
    "        if 'gfn_sim' not in gallery_output_dict:\n",
    "            gallery_output_dict['gfn_sim'] = []\n",
    "        gallery_output_dict['gfn_sim'].append(qg_scene_sim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e063faf7",
   "metadata": {},
   "source": [
    "## Display Search and Re-ID Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03fffc57",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Compute subplot width ratios\n",
    "width_list = [query_tensor_list[0].shape[2]] + [g.shape[2] for g in gallery_tensor_list]\n",
    "width_ratios = [w/sum(width_list) for w in width_list]\n",
    "kw = dict(width_ratios=width_ratios)\n",
    "\n",
    "# For each query image\n",
    "for query_idx, (query_tensor, query_detect) in enumerate(zip(query_tensor_list, query_output_list)):\n",
    "    # Initialize subplots\n",
    "    detect_fig, ax = plt.subplots(nrows=1, ncols=len(width_list), figsize=(20, 12), gridspec_kw=kw)\n",
    "  \n",
    "    # Show query detects\n",
    "    show_detects(query_tensor, query_detect, ax=ax[0], title='Query')\n",
    "\n",
    "    # Get query embeddings\n",
    "    query_crop_list = get_crops(query_tensor, query_detect)\n",
    "\n",
    "    # For each gallery image\n",
    "    full_crop_list, full_score_list = query_crop_list, []\n",
    "    for i, gallery_output_dict in enumerate(gallery_output_list):\n",
    "        ## Get person sim\n",
    "        person_sim = gallery_output_dict['person_sim'][query_idx]\n",
    "        \n",
    "        ## Get query-scene sim\n",
    "        qg_scene_sim = gallery_output_dict['gfn_sim'][query_idx]\n",
    "        \n",
    "        ## Show gallery detects\n",
    "        show_detects(gallery_tensor_list[i], gallery_output_dict, ax=ax[i+1], person_sim=person_sim,\n",
    "            title='Gallery {}'.format(i+1), xlabel='GFN Score: {:.2f}'.format(qg_scene_sim))\n",
    "        \n",
    "        ## Get image crops\n",
    "        crop_list = get_crops(gallery_tensor_list[i], gallery_output_dict)\n",
    "        score_list = person_sim.tolist()\n",
    "        full_crop_list.extend(crop_list)\n",
    "        full_score_list.extend(score_list)\n",
    "        \n",
    "    # Save detect fig\n",
    "    detect_fig.tight_layout()\n",
    "    detect_fig.savefig(f'query_detect{query_idx}.png', bbox_inches='tight', dpi=100)\n",
    "        \n",
    "    # Show ranked re-id crops\n",
    "    reid_fig = show_reid(full_crop_list, full_score_list)\n",
    "    \n",
    "    # Save re-id fig\n",
    "    reid_fig.tight_layout()\n",
    "    reid_fig.subplots_adjust(wspace=0.0)\n",
    "    reid_fig.savefig(f'query_reid{query_idx}.png', bbox_inches='tight', dpi=100)\n",
    "        \n",
    "# Adjust subplot spacing and show plots\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
